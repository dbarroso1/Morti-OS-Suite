{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Cleaning the Corpus and making the Dictionaries the Seq2Seq Model will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\morti_os_suite\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Importing the Libraries\n",
    "import re, time, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con(var,num):\n",
    "    limit = num\n",
    "    print(\"Size: {}\".format(len(var)))\n",
    "    print(\"Sample:\"+\"\\n\"+\"{}\".format(var[1:limit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Variables\n",
    "id2line = {}\n",
    "conv_ids = []\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "clean_q = []\n",
    "clean_a = []\n",
    "\n",
    "w2c = {}\n",
    "\n",
    "# hyper parameters\n",
    "threshold = 24\n",
    "qw2int = {}\n",
    "aw2int = {}\n",
    "word_num = 0\n",
    "\n",
    "tokens = ['<PAD>','<EOS>','<OUT>','<SOS>']\n",
    "\n",
    "q2int = []\n",
    "a2int = []\n",
    "\n",
    "s_clean_q = []\n",
    "s_clean_a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Cornell Movie Data\n",
    "lines = open('data/movie_lines.txt', \n",
    "             encoding='utf-8',\n",
    "             errors='ignore').read().split('\\n')\n",
    "conversations = open('data/movie_conversations.txt', \n",
    "             encoding='utf-8',\n",
    "             errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dictionary that maps the line to its ID\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of all the conversations\n",
    "for conversation in conversations[:-1]:\n",
    "    _conv = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    conv_ids.append(_conv.split(','))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the questions and Answers to individual lists\n",
    "for conv in conv_ids:\n",
    "    for i in range(len(conv) - 1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaing the Dictionaries\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"y'know\", \"you know\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"\\!\", \"\", text)\n",
    "    text = re.sub(r\"\\...\", \"\", text)\n",
    "    text = re.sub(r\"!@#$%^&*()_+-=`~;',./:<>?\", \"\", text)\n",
    "    text = re.sub(r\"0123456789\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    clean_q.append(clean_text(question))\n",
    "for answer in answers:\n",
    "    clean_a.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary that maps each word to its number of ocurrences'\n",
    "for q in clean_q:\n",
    "    for word in q.split():\n",
    "        if word not in w2c:\n",
    "            w2c[word] = 1\n",
    "        else:\n",
    "            w2c[word] += 1\n",
    "            \n",
    "for a in clean_a:\n",
    "    for word in a.split():\n",
    "        if word not in w2c:\n",
    "            w2c[word] = 1\n",
    "        else:\n",
    "            w2c[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, count in w2c.items():\n",
    "        if count >= threshold:\n",
    "            qw2int[word] = word_num\n",
    "            word_num += 1\n",
    "for word, count in w2c.items():\n",
    "        if count >= threshold:\n",
    "            aw2int[word] = word_num\n",
    "            word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the last tokens to theres 2 dictionaries\n",
    "for token in tokens:\n",
    "    qw2int[token] = len(qw2int) + 1\n",
    "for token in tokens:\n",
    "    aw2int[token] = len(aw2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the inverse dictionary of the answerswords2int dictionary\n",
    "ai2word = {w_i: w for w, w_i in aw2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the End Of String token to the end of every answer\n",
    "for i in range(len(clean_a)):\n",
    "    clean_a[i] += '<EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating all the questions and the answers into integers\n",
    "# and Replacing all the words that were filtered out by <OUT> \n",
    "for q in clean_q:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in qw2int:\n",
    "            ints.append(qw2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(qw2int[word])\n",
    "    q2int.append(ints)\n",
    "    \n",
    "for a in clean_a:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in aw2int:\n",
    "            ints.append(aw2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(aw2int[word])\n",
    "    a2int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting Questions and Answers by the length of the question\n",
    "for length in range(1,25 + 1):\n",
    "    for i in enumerate(q2int):\n",
    "        if len(i[1]) == length:\n",
    "            s_clean_q.append(q2int[i[0]])\n",
    "            s_clean_a.append(a2int[i[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model\n",
    "Functions that will define the architecture of the Seq2Seq Model\n",
    "\n",
    "Here we are making the Tensorflow Placeholders, Variables, and Graphs needed for the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatinng Placeholders for the inputs and Targets\n",
    "def model_inputs():\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32,[None,None],name='input')\n",
    "    \n",
    "    targets = tf.placeholder(tf.int32,[None,None],name='target')\n",
    "    \n",
    "    # Hyper Params\n",
    "    lr = tf.placeholder(tf.float32,name='learning_rate')\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the targets\n",
    "def preprocess_targets(targets,w2c,batch_size):\n",
    "    \n",
    "    left_side = tf.fill([batch_size,1],w2c['<SOS>'])\n",
    "    \n",
    "    right_side = tf.strided_slice(targets,[0,0],[batch_size,-1],[1,1])\n",
    "    \n",
    "    preporcessed_targets = tf.concat([left_side,right_side],1)\n",
    "    \n",
    "    return preporcessed_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the Encoder RNN Layer\n",
    "# Creating the LSTM Cell with Dropout\n",
    "def encoder_rnn_layer(rnn_inputs, rnn_size, num_layers, keep_prob, seq_len):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    _, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw= encoder_cell,\n",
    "                                                       cell_bw= encoder_cell,\n",
    "                                                       sequence_length= sequence_length,\n",
    "                                                       inputs= rnn_inputs,\n",
    "                                                       dtype=tf.float32)\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell Below needs updating for Tensorflow 1.7 API \n",
    "[Stack Overflow Question for Above](https://stackoverflow.com/questions/50518926/ideas-for-upgrading-code-from-tensorflow-v1-0-12-to-1-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the Training set\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_func, keep_prob, batch_size):\n",
    "    attn_state = tf.zeros([batch_size,1,decoder_cell.output_size])\n",
    "    \n",
    "    attn_keys, attn_values, attn_score_func, attn_construct_func = tf.contrib.seq2seq.prepare_attention(attention_states, \n",
    "                                                                                                        atention_option='bahdanau',\n",
    "                                                                                                        num_units=decoder_cell.output)\n",
    "\n",
    "    training_decoder_func = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                          attn_keys,\n",
    "                                                                          attn_values,\n",
    "                                                                          attn_score_func,\n",
    "                                                                          attn_construct_func,\n",
    "                                                                          name = \"attn_dec_train\")\n",
    "    \n",
    "    decoder_output, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                  training_decoder_func,\n",
    "                                                                  decoder_embedded_input,\n",
    "                                                                  sequence_length,\n",
    "                                                                  scope= decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output,keep_prob)\n",
    "    return output_func(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
